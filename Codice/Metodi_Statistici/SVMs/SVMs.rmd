---
title: "R Notebook"
output: html_notebook
---

```{r}
library(readr)

cell_file <- "Codice/Metodi_Statistici/lagged_datasets/Cella_15.csv"
cell_number <- 15

#leggiamo il dataset della cella
cella <- read_csv(cell_file)
cella$Date <- NULL  # rimuove la prima colonna dato che ha l'orario
spec(cella) # per vedere se ci sono problemi
```

```{r}
# fix colonne
cella$PompaGlicoleMarcia <- as.factor(cella$PompaGlicoleMarcia)
cella$Raffreddamento <- as.factor(cella$Raffreddamento)
cella$VentilatoreMarcia <- as.factor(cella$VentilatoreMarcia)

cella$TemperaturaMandataGlicoleNominale <- NULL

```

Type any R code in the chunk, for example:
```{r}
library(tidyverse)

cella <- as_tibble(cella)
summary(cella) # assicuriamoci che vada tutto# bene
```

```{r}
library(Metrics)

# suddivisione
DATASET_SIZE <- nrow(cella)

TEST_SIZE <- 0.2
# TUNING_SIZE <- 0.8

K_FOLDS <- 10

id_test <- as.integer(DATASET_SIZE * (1 - TEST_SIZE)):DATASET_SIZE

cella_test <- cella[id_test,]
cella <- cella[-id_test,]

# per i metodi che richiedono x e y
x_train <- cella %>% select(-TemperaturaCelle)
x_train <- data.matrix(x_train)
y_train <- cella %>% select(TemperaturaCelle)
y_train <- unlist(y_train)

x_test <- cella_test %>% select(-TemperaturaCelle)
x_test <- data.matrix(x_test)
y_test <- cella_test %>% select(TemperaturaCelle)
y_test <- unlist(y_test)

#                   shuffle
partitions <- split(sample(seq_len(nrow(cella))), cut_number(seq_len(nrow(cella)), K_FOLDS, labels = FALSE))



createDFs <- function(df, partitions, partition_number = 1) {
  # partizione indicata
  valid <- df[unlist(partitions[partition_number]),]
  # tutte le partizioni tranne quella indicata
  tr <- df[-unlist(partitions[partition_number]),]
  return(list(training = tr, validation = valid))
}

compute_metrics <- function(name = "dummy", preds = c(2, 1, 3, 8, 8, 2, 3, 6), test = c(7, 3, 4, 5, 6, 8, 5, 6)) {
  error <- data.frame(name = name,
                      mse = mse(actual = test, predicted = preds), # mean square error
                      rmse = rmse(actual = test, predicted = preds), # root mean square error
                      rmsle = rmsle(actual = test, predicted = preds), # root mean square log error
                      rrse = rrse(actual = test, predicted = preds), # root relative square error
                      rse = rse(actual = test, predicted = preds), # relative square error
                      bias = bias(actual = test, predicted = preds), # bias del modello
                      pbias = percent_bias(actual = test, predicted = preds), # percent bias
                      mae = mae(actual = test, predicted = preds), # Mean Absolute Error
                      rae = rae(actual = test, predicted = preds), # relative absolute error
                      mape = mape(actual = test, predicted = preds), # Mean Absolute Percentage Error
                      mdae = mdae(actual = test, predicted = preds), # Median Absolute Different Error
                      msle = msle(actual = test, predicted = preds), # Mean Squared Log Error
                      smape = smape(actual = test, predicted = preds), #symmetric mean absolute percentage error
                      sse = sse(actual = test, predicted = preds), # sum of the squared differences between two numeric vectors
                      r2 = cor(test, preds)^2
  )

  return(error)
}

errors <- compute_metrics()


```

# Maximal Margin Classifier
```{r}
library(e1071)
# no need for tuning

# the budget cannot be set to 0, so the cost will be set to a very high
model_mmc <- svm(TemperaturaCelle ~ ., data = cella, kernel = "linear", cost = 1e100,
                 scale = FALSE, type = "eps-regression")
summary(model_mmc)
```

```{r}
predictions <- predict(model_mmc, cella_test)
errors <- rbind(compute_metrics(name = "Simple Tree", preds = predictions, test = cella_test$TemperaturaCelle))
```


```{r}

```

```{r}
library(ggplot2)

x <- seq_along(predictions)
df <- data.frame(x, y1 = predictions, y2 = cella_test$TemperaturaCelle)

ggplot(df, aes(x)) +                    # basic graphical object
  geom_line(aes(y = y1), colour = "black") +  # first layer
  geom_line(aes(y = y2), colour = "green") + # second layer
  labs(title = "Temperatura cella", subtitle = "Predetta vs attuale") +
  xlab("Tempo") +
  ylab("Temperatura")
```

## Pruning
```{r}

cv_train <- cv.tree(model, , prune.tree)
# 10 fold cv
plot(cv_train$size, cv_train$dev, type = "b", main = 'Cross-validation: first batch result',
     xlab = 'Tree size', ylab = 'Cross validation error')

# print optimal size
best_size <- cv_train$size[which.min(cv_train$dev)]
paste('Optimal size:', best_size)
saveRDS(best_size, file = paste0("Codice/Metodi_Statistici/saved_parameters/Trees/Pruned_Tree_best_size_cella_",cell_number))
```
```{r}
model_pruned <- prune.tree(model, best = best_size)
plot(model_pruned)
text(model_pruned, pretty = 0)
summary(model_pruned)
```

```{r}
predictions <- predict(model_pruned, cella_test)
errors <- rbind(errors, compute_metrics(
  name = paste0("Pruned Tree (best size: ", best_size),
  preds = predictions,
  test = cella_test$TemperaturaCelle))
```

```{r}
library(ggplot2)

x <- seq_along(predictions)
df <- data.frame(x, y1 = predictions, y2 = cella_test$TemperaturaCelle)

ggplot(df, aes(x)) +                    # basic graphical object
  geom_line(aes(y = y1), colour = "black") +  # first layer
  geom_line(aes(y = y2), colour = "green") + # second layer
  labs(title = "Temperatura cella", subtitle = "Predetta vs attuale") +
  xlab("Tempo") +
  ylab("Temperatura")
```

## Bagging

```{r}
# NB: Takes a lot
library(randomForest)
max_predictors <- ncol(cella) - 1
model_bagging <- randomForest(TemperaturaCelle ~ ., data = cella, ntree = MAX_TREES, mtry = max_predictors)
plot(model_bagging)
```

```{r}
predictions <- predict(model_bagging, cella_test)
errors <- rbind(errors, compute_metrics(
  name = paste0("Bagged Tree (N_trees: ", MAX_TREES),
  preds = predictions,
  test = cella_test$TemperaturaCelle))
```

```{r}
library(ggplot2)

x <- seq_along(predictions)
df <- data.frame(x, y1 = predictions, y2 = cella_test$TemperaturaCelle)

ggplot(df, aes(x)) +                    # basic graphical object
  geom_line(aes(y = y1), colour = "black") +  # first layer
  geom_line(aes(y = y2), colour = "green") + # second layer
  labs(title = "Temperatura cella", subtitle = "Predetta vs attuale") +
  xlab("Tempo") +
  ylab("Temperatura")
```

## Random forest

```{r}
# # Ci mette troppo. Ci conviene usare il default
# mse <- NULL
#
# for (m in seq_len(max_predictors - 1)) {
#   rf_tmp <- randomForest(TemperaturaCelle ~ ., data = cella, mtry = m, ntree = MAX_TREES)
#   mse <- c(mse, mean(rf_tmp$mse))
# }
#
# minim <- which.min(mse)
# saveRDS(minim, file = paste0("Codice/Metodi_Statistici/saved_parameters/Trees/rf_best_size_cella_", cell_number))
```
```{r}
library(randomForest) # p/3 predittori
model_rf <- randomForest(TemperaturaCelle ~ ., data = cella, ntree = MAX_TREES)
plot(model_rf)
varImpPlot(model_rf)
```

```{r}
predictions <- predict(model_bagging, cella_test)
errors <- rbind(errors, compute_metrics(
  name = paste0("Random Forest (N_trees: ", MAX_TREES),
  preds = predictions,
  test = cella_test$TemperaturaCelle))
```

```{r}
library(ggplot2)

x <- seq_along(predictions)
df <- data.frame(x, y1 = predictions, y2 = cella_test$TemperaturaCelle)

ggplot(df, aes(x)) +                    # basic graphical object
  geom_line(aes(y = y1), colour = "black") +  # first layer
  geom_line(aes(y = y2), colour = "green") + # second layer
  labs(title = "Temperatura cella", subtitle = "Predetta vs attuale") +
  xlab("Tempo") +
  ylab("Temperatura")
```

## Boosting
```{r}
library(caret)

# reference: https://rpubs.com/mpfoley73/529130

# re_encode variables
boosting_model <- train(TemperaturaCelle ~ .,
                        data = cella,
                        method = "gbm",  # for bagged tree
                        tuneLength = 5,  # choose up to 5 combinations of tuning parameters
                        metric = "mse",  # evaluate hyperparamter combinations with ROC
                        trControl = trainControl(
                          method = "cv",  # k-fold cross validation
                          number = K_FOLDS,  # 10 folds
                          savePredictions = "final"      # save predictions for the optimal tuning parameter1
                        )
)
# caret tunes:
#    n.trees: number of boosting iterations
#    interaction.depth: maximum tree depth
#    shrinkage: shrinkage
#   n.minobsinnode: mimimum terminal node size

```
```{r}
plot(boosting_model)
```

```{r}
predictions <- predict(boosting_model, cella_test)
errors <- rbind(errors, compute_metrics(
  name = paste0("Boosted Trees"),
  preds = predictions,
  test = cella_test$TemperaturaCelle))
```

```{r}
library(ggplot2)

x <- seq_along(predictions)
df <- data.frame(x, y1 = predictions, y2 = cella_test$TemperaturaCelle)

ggplot(df, aes(x)) +                    # basic graphical object
  geom_line(aes(y = y1), colour = "black") +  # first layer
  geom_line(aes(y = y2), colour = "green") + # second layer
  labs(title = "Temperatura cella", subtitle = "Predetta vs attuale") +
  xlab("Tempo") +
  ylab("Temperatura")
```


# BART (Bayesian Additive Regression Trees)

NB: bart wants the test in the call, so this is just a display


```{r}
library(BART)
wbart_model <- mc.wbart(x_train, y_train, mc.cores = 8, x.test = x_test)
```
```{r}
errors <- rbind(errors, compute_metrics(
  name = paste0("BART"),
  preds = wbart_model$yhat.test.mean,
  test = cella_test$TemperaturaCelle))
```
```{r}
library(ggplot2)

x <- seq_along(predictions)
df <- data.frame(x, y1 = predictions, y2 = cella_test$TemperaturaCelle)

ggplot(df, aes(x)) +                    # basic graphical object
  geom_line(aes(y = y1), colour = "black") +  # first layer
  geom_line(aes(y = y2), colour = "green") + # second layer
  labs(title = "Temperatura cella", subtitle = "Predetta vs attuale") +
  xlab("Tempo") +
  ylab("Temperatura")
```

```{r}
saveRDS(errors, file = paste0("Codice/Metodi_Statistici/saved_parameters/metriche_errori/Alberi_cella_",cell_number,".rds"))
```
